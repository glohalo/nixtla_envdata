---
title: "Forecasting kNDVI using Ridge Regression and TimeGPT"
author: "Gloria Carrascal"
date: today
bibliography: references.bib

format:
    html:
        polyfill: false
        embed-resources: true
        toc: true        
        toc-location: left
        toc-title: "Contents"
        toc-depth: 3
        number-sections: true
        css: style.css
        page-layout: article 
---

# Introduction 

In ecosystem monitoring, the study of vegetation dynamics reflects the health and resilience of the place. This report presents a forecasting analysis of the Kernel Normalized Difference Vegetation Index (kNDVI) 
using two modeling approaches: a baseline Ridge Regression model and the TimeGPT model. The work includes 
exploratory data analysis, forecasting, and performance evaluation using MAE.
Analyzing these dynamics can reveal the impacts of environmental changes like climate change, human activities, 
and natural disturbances, informing conservation efforts and sustainable management practices.

# Data and Methods

This section describes the dataset used in the study, the preprocessing steps applied, and the forecasting methods implemented. It begins by introducing the kNDVI dataset and its structure, followed by a 
description of the forecasting models employed to generate predictions and evaluate their performance.


## Dataset Description

The dataset comprises Earth Observations as time series. It consist of 214,351 time series for the trainning dataset. The files used during the challenge were split into train and validation subsets as shown in @tbl-summarydataset.

| Dataset| Unique Ids   | Timesteps|  NaNs|
|------|------|------|------|
| Train   | 214,751    | 1004   | 0|
| Validation  | 71,755   | 1004   | 0 |

: Datasets available to develop the project, summary of the basics shapes and NaN counts. {#tbl-summarydataset}

The data contains numpy arrays with dimensions ```[batch, time, variables]```.
The variables are given in the next order: ```[air temperature, evaporation, 
precipitation, radiation, kNDVI]```. These values correspond to an index from 0 to 4. 
In this projecj is used the index 4. Regarding the time, the historical data corresponds
to the period from 2000-03-01 to 2021-12-27 with a timestep of 8 days ^[[kNDVI Prediction Challenge, documentation](https://git.sc.uni-leipzig.de/ss2025std/kndvi-prediction-challenge)] .

## Models 

In order to create the forecast of the kNDVI in a time horizont of 92 days. The transformer **TimeGPT** 
is the principal tool. However, in the project as a baseline the **Ridge Regression** is used
to compare if **TimeGPT** acts as a game changer for *multiforecasting* environmental data. Various evaluation metrics are used, including Mean Squared Error (MSE), 
Root Mean Squared Error (RMSE), and Mean Average Error (MAE). Each metric offers a distinct viewpoint on the model’s performance 
and plays a crucial role in determining its effectiveness in predicting the kNDVI. 

### Ridge Regression 

This method consists of estimating the coefficients of multiple-regression models. It works especially 
under scenarios where independent variables have a high correlation. The model minimizes the objective function in @eq-regressor ,
but with a twist: it includes **L2 regularization**. This means it not only tries to reduce the error
between the predicted and actual values, but it also adds a penalty if the model's coefficients become too large. Also known 
as Ridge Regression or Tikhonow regularization [@ridgewikipedia;@ridgesklearn].

$$ 
 ||y -Xw||^2_2 + alpha *||w||^2_2
$$ {#eq-regressor}

In the context of the kNDVI challenge, the ridge regressor model is applied with an univariate time series forecasting approach. Only the target variable, the 
kNDVI and its time column. However, the ridge regressor doesn't natively understand time or seasonality [@ridgewikipedia], it was created with a single month lag as an exogenous
variable to predict the next 92 timesteps. 

### Nixtla TimeGPT

**TimeGPT** is a production-ready generative pre-trained transformer model specifically designed for time series forecasting. The @fig-timegpt describes the fine-tuning process architecture from TimeGPT
to create the forecast [@timegptp_architecture]. In this project, the few-shot learning framework is used. Few-shot learning consists of training an AI model on a very small number of samples [@ibm-fewlearning]. In this case, 
with TimeGPT, a fine-tuning process is employed to adapt the model to the specific forecast task. Additionally, 
exogenous variables were given to generate the 92 predictions, ```['month', 'quarter', 'year', 'dayofyear']```.

 ![Architecture of the model and fine tuning process involved in TimeGPT to forecast a time series [@timegptp_architecture].](figures/timegpt_diagram.jpg){#fig-timegpt}

The fine-tuning process is carried out using the full historical load data (see @fig-timegpt). This data is used to update the weights of all layers in the model.
The model processes all past values using *self-attention* and builds a latent representation of the full historical context, including trends and seasonal behavior. 
As the horizon of predictions was long with few years of context the generalizations were poor, the time series variability was trackless.


# Results and Discussion

In this this section we explore the analysis of the two approaches under a spatial validation setting. The first method is based on a simple ridge regression model, 
while the second uses TimeGPT, a transformer-based model designed to handle long-range temporal dependencies. Both approaches aim to predict the kNDVI signal over 
a 92-step horizon, but they work in fundamentally different ways.

## Model Performance

Model performance was evaluated over a validation period from 2020-02-21 to 2022-02-10, covering 92 time steps at 8-day intervals. 
The @tbl-forecastmetrics summarizes the average MAE, RMSE, and MSE for Ridge Regression and multiple TimeGPT configurations:

The @tbl-forecastmetrics shows that,the ridge regression model adopts a straightforward approach. Each pixel is treated independently, and the model uses only one lagged value. This method 
does not require any spatial beyond the context of each time series and its lagged value. Due to this simplicity,
Ridge Regression naturally fits a spatial cross-validation setting: each point can be trained and evaluated independently
without any dependency on neighboring points. While this approach is computationally simple and somewhat robust, it lacks the capacity to capture more complex temporal 
patterns such as seasonality or long-term dependencies.

One of the main goals when modeling spatio-temporal data is to evaluate whether a model can generalize to unseen spatial locations. This ability, often referred to as 
spatial generalization, is only meaningful if the setup strictly separates training and validation points in space [@def_spatiotemporalmodel]. If validation data is mixed 
model may learn patterns from locations that are supposed to be held out, resulting in overly optimistic evaluation metrics. As a result, some validation points had to be included in the training phase which
means the "context data" for TimeGPT model, and a strict test of spatial generalization could not be achieved. TimeGPT expects a history of values for each spatial point, organized under unique identifiers. During forecasting, 
it predicts future values conditioned on this historical context. However, in this project, the validation set contained entirely different spatial points not present in 
the training set. Since TimeGPT cannot forecast future values without past observations for each point, we had to include the historical part of the validation 
series as part of the input. This means the model had access to validation locations during inference, making it impossible to evaluate pure spatial generalization in this 
setup.



To evaluate the model performance of the Ridge Regressor and TimeGPT forecast, the validation dataset was used. The prediction period corresponds from 2020-02-21 to 2022-02-10, 
a total of 92 time steps with a frequency of 8 days. The average evaluation metrics of Ridge Regressor and TimeGPT are shown in @tbl-forecastmetrics.


| Model | MAE   | RMSE | MSE |
|------|------|------|------|
| Ridge Regressor    | 0.1031    | 0.1048   | 0.1031|
| TimeGPT not fine-tuning  (exog)| 0.0430   | 0.0974    | 0.0095 |
| TimeGPT with fine-tuning (exog) | 0.0242   | 0.1665    | 0.0277 |
| TimeGPT not fine-tuning  not exog| 0.0620   | 0.1391    | 0.0193 |
| TimeGPT with fine-tuning  not exog| 0.0249   | 0.1002    | 0.0100 |
| TimeGPT Half context with fine-tuning (exog)  | 0.1618   | 0.2480    | 0.0615 |
| TimeGPT Half context not fine-tuning (exog) | 0.1571   | 0.2427    | 0.0589 |
| TimeGPT Half context no exog, not fine-tuning| 0.1521   | 0.2441   | 0.0596 |
| TimeGPT Half context no exog, fine-tuning| 0.1609   | 0.2465   | 0.0608 |

: Forecast metrics for Ridge Regressor and TimeGPT models during the prediction of 220,000 time series {#tbl-forecastmetrics}

From a practical standpoint, Ridge Regression offers simplicity and interpretability. It is a strong baseline, especially in 
scenarios where computational efficiency or explainability is prioritized. TimeGPT, in contrast, demonstrates superior performance 
when full historical context is available and when the goal is to capture complex temporal dependencies. The trade-off lies in model 
complexity and the increased attention needed to avoid data leakage.

As shown in @tbl-forecastmetrics, models like the Ridge Regressor serves as a basic benchmark with moderate error metrics. Notably, 
TimeGPT outperforms it even without fine-tuning, highlighting its capacity to model richer temporal structures. When fine-tuned—adjusting its transformer 
layers to minimize the mean squared error—TimeGPT's performance improves substantially. As a result, TimeGPT with fine-tuning achieves the lowest MAE, 
establishing it as a highly effective model for vegetation forecasting. Experiments using only half of the available historical context shows an evident drop 
in performance (See @tbl-forecastmetrics). This confirms that the model strongly depends on 
having long historical sequences available per spatial point for time series such the kNDVI signal, which are characterized by high temporal
variability, irregular sampling, and missing or interpolated observations. The inclusion of exogenous variables helps in some cases, but the key driver of performance remains the length of 
the input history. These results suggest that TimeGPT is best suited for dense temporal coverage, and may not generalize well to sparse or truncated histories. This has implications 
for applying transformer models in environmental forecasting, where data availability can vary across space and time.


## Forecast Shape Analysis

Beyond numerical error metrics, evaluating the shape and structure of forecasted time series is essential to understanding how well models
capture the dynamics and patterns of the target variable. Visual inspection enables the assessment f qualitative aspects; such as trends, fluctuations, and seasonal behaviors
which are not fully reflected by aggregate error values. Starting with Ridge Regression, @fig-ridgeressorreal
displays kNDVI forecasts for four spatial points (pixels). While the model yields moderately low error metrics (@tbl-forecastmetrics), this figure highlights a critical limitation: 
the predicted time series often fail to replicate the actual signal’s temporal structure. Instead, the forecasts converge to flat or increased line pattenr which are disconnected from the observed kNDVI dynamics. 
This illustrates how models like Ridge Regression may produce deceptively “good” average errors while missing critical variations in the signal.


::: {#fig-ridgeressorreal layout-nrow=2}
![Pixel 381.](figures/pixel_381.png)

![Pixel 408.](figures/pixel_409.png)

![Pixel 560.](figures/pixel_560.png)

![Pixel 645.](figures/pixel_645.png)

Ridge Regression predictions (dashed green lines) versus actual kNDVI values (green) across four sample pixels. 
The model captures basic trends but fails to account for complex seasonal or irregular patterns present in the signal.
:::

In contrast to Ridge Regression (see, @fig-ridgeressorreal), @fig-timegptshortcontextnotexognotfinetuning presents 
TimeGPT forecasts under a constrained configuration: short historical context, no exogenous variables, and no fine-tuning. Despite 
the architectural complexity of TimeGPT, this setup limits its capacity to extract meaningful temporal patterns. As shown, 
the model produces flat trending predictions that are detached from the actual kNDVI signal. However, while Ridge achieves better error metrics (MAE = 0.1031), it does so by over-smoothing the forecast into a constant or increasing trend. TimeGPT, 
on the other hand, achieves higher MAE (0.1521) in this limited configuration, revealing its dependency on rich historical context and model adaptation on long horizont predicitons, for effective performance. In the absence of sufficient temporal depth or contextual information, 
even powerful sequence models like TimeGPT revert to weak baselines, and may perform worse than simpler linear methods.

::: {#fig-timegptshortcontextnotexognotfinetuning layout-nrow=2}
![Unique ID 62338.](figures/62338_shortcontextNoexo_Nofinetuning.png)

![Unique ID 145133.](figures/145133_shortcontextNoexo_Nofinetuning.png)

![Unique ID 162170.](figures/162170_shortcontextNoexo_Nofinetuning.png)

![Unique ID 162245.](figures/162245_shortcontextNoexo_Nofinetuning.png)

TimeGPT predictions (dashed green line) compared to actual kNDVI values (solid green line) for selected spatial points. In this configuration, short context, 
not exogenous variables, and not fine-tuning were applied.
:::

The comparison between two configurations of TimeGPT, one without fine tuning and one with fine tuning and exogenous 
variables presents differences in the model’s ability to capture the temporal dynamics of the kNDVI signal. In the first case, 
without fine tuning, the model tends to produce forecasts that regress toward the mean of the series. As shown in @fig-timegptnofinetuning, 
predictions often start near the average value and loosely follow the overall trend, but fail to reflect finer temporal details such as local deviations, 
seasonal peaks, or sudden changes. This results in an over-smoothed and less informative reconstruction of the time series. In contrast, the fine tuned configuration 
with exogenous variables, illustrated in @fig-timegptfinetuning, shows significant improvements: forecasts initiate closer to the observed values and follow the structure 
of the series more accurately. This includes improved reproduction of sharp transitions, amplitude variations, and localized fluctuations. These behaviors demonstrate the model’s capacity 
to adapt its internal representations to the specific patterns of the kNDVI signal after fine tuning. Despite having similar average error values (see @tbl-forecastmetrics), the qualitative 
behavior of the forecasts diverges substantially. This reinforces a key insight: TimeGPT’s effectiveness in long-horizon forecasting is highly dependent on the availability of long historical context and model 
adaptation. Without these elements, it behaves conservatively; with them, it can successfully reproduce the underlying signal dynamics.

::: {#fig-timegptfinetuning layout-nrow=2}
![Unique ID 85679.](figures/85679.png)

![Unique ID 91427.](figures/91427.png)

![Unique ID 148220.](figures/148220.png)

![Unique ID 165133.](figures/165133.png)

TimeGPT predictions (dashed green line) compared to actual kNDVI values (solid green line) for selected spatial points. This configuration 
includes all the context, exogenous variables, and fine-tuning.
:::

::: {#fig-timegptnofinetuning layout-nrow=2}
![Unique ID 30928.](figures/30928_nofinetuning.png)

![Unique ID 17193.](figures/17193_nofinetuning.png)

![Unique ID 36676.](figures/36676_nofinetuning.png)

![Unique ID 105133.](figures/105133_nofinetuning.png)

TimeGPT predictions (dashed green line) versus actual kNDVI values (solid green line) for four selected spatial points. Full historical, exogenous variables, and no fine-tuning was applied.
:::

What we take from this exercise is that, although TimeGPT is a pre trained model originally developed for time series forecasting, particularly in the financial 
domain [@timegpt]; applying it to kNDVI prediction posed a nontrivial, yet surmountable, challenge. Due to the scale of the dataset, 
which includes approximately 220,000 unique time series, forecasts had to be executed in batches of 20,000 unique IDs, resulting 
in eleven separate API calls. In contrast, the Ridge Regressor handled the full dataset in a single execution, requiring significantly less 
computational effort and no batching. However, the trade-off is evident: while TimeGPT is more computationally demanding, it delivers substantially 
better results in capturing the temporal dynamics and variability of the kNDVI signal, especially when is fine tuned. This underscores the value of using 
transformer based models for environmental forecasting tasks, provided that sufficient historical data and resources for adaptation are available.


## Technical Challenges

To predict the kNDVI signals, several preprocessing and computationals tricks had to be addressed. The first critial step during before the 
forecast was the transformation of the input data into TimeGPT's required format. This data structure may be flexible 
for general time series tasks, but lacks of native
support for geospatial structures such datacubes. In this project, each cube had to be uniquely identified 
by joining latitude and longitude coordinates into a string-based unique ID, and grouped into batches for each forecast. 
The second step during execution, was to send the API the data. The API displays a progressive computation pattern 
(e.g., from 0% to 100% across batches), reflecting the non trivial effort required to handle long sequences. 
Internally, the model issues a warning when the forecast horizon exceeds its pretrained limits:

```
WARNING - The specified horizon "h" exceeds the model horizon, this may lead to less accurate forecasts. 
Please consider using a smaller horizon.
```
This suggests that forecasting long sequences may stretch the model beyond its optimal capacity, which should be taken 
into account in production scenarios. Additionally, the model uses inferred frequencies and requires exogenous variables 
such as month, quarter, year, and dayofyear, but does not provide feature importance, limiting interpretability. 
Being able to assess which features matter most would allow users to reduce complexity by selecting only relevant 
exogenous inputs.

# Conclusions 

This study demonstrates that TimeGPT can effectively forecast the kNDVI signal when provided with sufficient historical context. Despite 
being originally developed for financial time series applications [@timegpt], the model exhibits strong adaptability to environmental data, 
achieving reliable long-horizon forecasts of up to 92 time steps. When fine tuned and enriched with exogenous variables, TimeGPT outperforms a Ridge Regression baseline across multiple metrics, 
while also more accurately capturing the shape, amplitude, and dynamics of the kNDVI signal.

From a modeling perspective, Ridge Regression offers simplicity, interpretability, and computational efficiency, making it a practical benchmark for large-scale 
applications. However, its inability to model complex temporal patterns limits its usefulness for detailed environmental forecasting. In contrast, TimeGPT excels 
when long historical sequences are available. Yet, the model's effectiveness is highly contingent on full context, and fine tuning. Without these elements, even a powerful transformer 
architecture may underperform and revert to generic or smoothed predictions.

The analysis also underscores that numerical error metrics alone are insufficient to evaluate model quality. Forecast shape analysis revealed that 
while Ridge Regression and non fine tuned TimeGPT may yield moderate error values, they fail to capture relevants signal characteristics such as seasonal fluctuations, 
abrupt changes, or localized anomalies. In contrast, the fine tuned version of TimeGPT accurately reconstructs these dynamics, demonstrating its potential for real world vegetation monitoring tasks.

This study also presents important technical challenges, forecasting with TimeGPT at scale required transforming gridded data into a flat format using string based unique identifiers 
and executing forecasts in batches due to API limits. This workaround does not scale well and highlights the need for forecasting APIs that natively support geospatial data formats 
(e.g., H3, S2, or lat/lon pair indexing). Furthermore, the model's reliance on inferred frequencies and pre-defined exogenous inputs, without offering feature importance diagnostics, 
limits transparency and interpretability. While the current TimeGPT framework is designed for univariate time series, future extensions could explore incorporating spatial context or 
generalization capabilities, enabling forecasts for previously unseen locations based on learned spatial relationships. This would open new possibilities for applying transformer-based models 
to spatio temporal problems in Earth observation. Finally, TimeGPT shows promising potential for **gap-filling and temporal interpolation** in satellite-derived time series, where missing observations 
due to cloud cover, sensor issues, or preprocessing errors are common. Once provided with full historical context, the model successfully generalizes the temporal dynamics, making it a valuable 
tool for Earth observation and remote sensing tasks, provided sufficient computational resources and contextual information available.

# Acknowledgement

This project was developed as part of the Time Series course in the Earth System Data Science and Remote Sensing Master’s program, 
within the context of the kNDVI Prediction Challenge. 
We gratefully acknowledge the support of Nixtla, who provided access to the TimeGPT API for experimentation and scientific exploration.
